{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exercises - Gradient Descent\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data points for a Regression problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "X, y = datasets.make_regression(n_samples=100, n_features=1,\n",
    "                                n_informative=1, noise=10.0,\n",
    "                                random_state=42)\n",
    "plot(X, y, 'bx')\n",
    "xlabel('$x_1$')\n",
    "ylabel(\"$y$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the data points for matrix manipulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ext = insert(X, 0, ones(len(X)), axis=1)\n",
    "Y = y.reshape(len(y), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 1. Ordinary Least Squares\n",
    "\n",
    "Find the weight values $\\mathbf{w}$ that minimize the error $E_{\\mathbf{in}}(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^n {(\\mathbf{w}^T \\mathbf{X}_n - \\mathbf{y}_n)^2}$.\n",
    "\n",
    "For this, implement Linear Regression and use the Ordinary Least Squares (OLS) closed-form expression to find the estimated values of $\\mathbf{w}$:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^{\\rm T}\\mathbf{X})^{-1} \\mathbf{X}^{\\rm T}\\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W = np.linalg.inv(X_ext.T.dot(X_ext)).dot(X_ext.T).dot(Y)\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot(X, y, 'bx')\n",
    "plot(X, X_ext.dot(W), 'r.')\n",
    "xlabel('$x_1$')\n",
    "ylabel(\"$y$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 2: Batch Gradient Descent\n",
    "\n",
    "Find the weight values $\\mathbf{w}$ that minimize the error $E_{\\mathbf{in}}(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^n {(\\mathbf{w}^T \\mathbf{X}_n - \\mathbf{y}_n)^2}$.\n",
    "\n",
    "For this, implement the Batch Gradient Descent algorithm with $\\mathbf{s}$ learning steps and learning rate $\\alpha$.  \n",
    "At each training step, update $\\mathbf{w}$ with this rule:\n",
    "\n",
    "$$\\mathbf{w}_i := \\mathbf{w}_i - \\alpha \\left(\\left(\\mathbf{X}\\mathbf{w} - \\mathbf{y}\\right)^T\\mathbf{X}_i\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, d = X_ext.shape\n",
    "s = 100 # learning steps\n",
    "alpha = 0.01 # learning rate\n",
    "\n",
    "W = zeros((d, 1))\n",
    "\n",
    "for step in range(s):\n",
    "    grad = (X_ext.dot(W) - Y).T.dot(X_ext).T\n",
    "    W = W - alpha * grad\n",
    "    print np.linalg.norm(grad)\n",
    "    if np.linalg.norm(grad) < 1e-4:\n",
    "        break\n",
    "\n",
    "print W\n",
    "\n",
    "plot(X, y, 'bx')\n",
    "plot(X, X_ext.dot(W), 'r.')\n",
    "xlabel('$x_1$')\n",
    "ylabel(\"$y$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution 3: Stochastic Gradient Descent\n",
    "\n",
    "Find the weight values $\\mathbf{w}$ that minimize the error $E_{\\mathbf{in}}(\\mathbf{w}) = \\frac{1}{N} \\sum_{n=1}^n {(\\mathbf{w}^T \\mathbf{X}_n - \\mathbf{y}_n)^2}$.\n",
    "\n",
    "For this, implement the Stochastic Gradient Descent algorithm with $\\mathbf{s}$ learning steps and learning rate $\\alpha$.\n",
    "In each step, iterate through all $j$ samples and, for each sample, update $\\mathbf{w}$ with this rule:\n",
    "\n",
    "$$\\mathbf{w}_i := \\mathbf{w}_i - \\alpha\\left(\\mathbf{X}^{(j)}\\mathbf{w} - \\mathbf{y}^{(j)}\\right)\\mathbf{X}^{(j)}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, d = X_ext.shape\n",
    "s = 20 # learning steps\n",
    "alpha = 0.1 # learning rate\n",
    "\n",
    "W = zeros((d, 1))\n",
    "\n",
    "for step in range(s):\n",
    "    for j in range(n):\n",
    "        grad = (X_ext[j].dot(W) - Y[j]).T.dot(X_ext[[j], :].reshape(1,d)).reshape(d,1)\n",
    "        W -= alpha * grad\n",
    "\n",
    "print W\n",
    "\n",
    "plot(X, y, 'bx')\n",
    "plot(X, X_ext.dot(W), 'r.')\n",
    "xlabel('$x_1$')\n",
    "ylabel(\"$y$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution 4: Gradient Descent with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model inputs: X and Y\n",
    "x_tensor = tf.placeholder(tf.float32)\n",
    "y_tensor = tf.placeholder(tf.float32)\n",
    "\n",
    "# define the model variables\n",
    "w_tensor = tf.Variable(np.zeros((X.shape[1], 1)), dtype=tf.float32)\n",
    "b_tensor = tf.Variable([0], dtype=tf.float32)\n",
    "\n",
    "# loss function to minimize: 1/n * (x.dot(w) + b - y)^2\n",
    "y_pred = tf.matmul(x_tensor, w_tensor) + b_tensor\n",
    "loss = tf.reduce_mean(tf.square(y_pred - y_tensor))\n",
    "\n",
    "# define the gradient descent step\n",
    "learning_rate = 0.5\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "feed_dict = {x_tensor: X, y_tensor: Y}\n",
    "\n",
    "# initialize session\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "\n",
    "try:\n",
    "    \n",
    "    for i in range(10):\n",
    "        sess.run(train_step, feed_dict=feed_dict)\n",
    "        print sess.run(loss, feed_dict=feed_dict)\n",
    "\n",
    "finally:\n",
    "    # collect results\n",
    "    W = sess.run(w_tensor, feed_dict=feed_dict)\n",
    "    B = sess.run(b_tensor, feed_dict=feed_dict)\n",
    "    sess.close()\n",
    "\n",
    "print W, B\n",
    "\n",
    "plot(X, y, 'bx')\n",
    "plot(X, X.dot(W)+B, 'r.')\n",
    "xlabel('$x_1$')\n",
    "ylabel(\"$y$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
