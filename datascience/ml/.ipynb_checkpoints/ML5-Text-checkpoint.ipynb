{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"http://www.bigdive.eu/wp-content/uploads/2012/05/logoBIGDIVE-01.png\">\n",
    "</center>\n",
    "\n",
    "---\n",
    "\n",
    "# Text Analysis\n",
    "\n",
    "## André Panisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "### The 20 Newsgroups data set\n",
    "\n",
    "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups\n",
    "\n",
    "The data is organized into 20 different newsgroups, each corresponding to a different topic. Some of the newsgroups are very closely related to each other (e.g. comp.sys.ibm.pc.hardware / comp.sys.mac.hardware), while others are highly unrelated (e.g misc.forsale / soc.religion.christian)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "dataset = datasets.fetch_20newsgroups()\n",
    "documents = dataset.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "Removing from text pieces that do not convey any semantic meaning (e.g., mail headers, email adresses, host names...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from_re = re.compile(r\"^From: .*\\n\")\n",
    "\n",
    "print from_re.sub('', documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(documents)):\n",
    "    documents[i] = from_re.sub('', documents[i])\n",
    "print documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: create a regular expression to remove the Nntp-Posting-Host header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Text Messages to Feature Vectors\n",
    "We need to transform our text data into feature vectors, numerical representations which are suitable for performing statistical analysis. The most common way to do this is to apply a bag-of-words approach where the frequency of an occurrence of a word becomes a feature for our classifier.\n",
    "\n",
    "\n",
    "## Term Frequency-Inverse Document Frequency\n",
    "\n",
    "We want to consider the relative importance of particular words, so we'll use term frequency–inverse document frequency as a weighting factor. This will control for the fact that some words are more \"spamy\" than others.\n",
    "\n",
    "## Mathematical details\n",
    "\n",
    "tf–idf is the product of two statistics, term frequency and inverse document\n",
    "frequency. Various ways for determining the exact values of both statistics\n",
    "exist. In the case of the '''term frequency''' tf(''t'',''d''), the simplest\n",
    "choice is to use the ''raw frequency'' of a term in a document, i.e. the\n",
    "number of times that term ''t'' occurs in document ''d''. If we denote the raw\n",
    "frequency of ''t'' by f(''t'',''d''), then the simple tf scheme is\n",
    "tf(''t'',''d'') = f(''t'',''d''). Other possibilities\n",
    "include:\n",
    "\n",
    "  * boolean_data_type \"frequencies\": tf(''t'',''d'') = 1 if ''t'' occurs in ''d'' and 0 otherwise; \n",
    "  * logarithmically scaled frequency: tf(''t'',''d'') = log (f(''t'',''d'') + 1); \n",
    "  * augmented frequency, to prevent a bias towards longer documents, e.g. raw frequency divided by the maximum raw frequency of any term in the document: :$\\mathrm{tf}(t,d) = 0.5 + \\frac{0.5 \\times \\mathrm{f}(t, d)}{\\max\\{\\mathrm{f}(w, d):w \\in d\\}}$\n",
    "\n",
    "The '''inverse document frequency''' is a measure of whether the term is\n",
    "common or rare across all documents. It is obtained by dividing the total\n",
    "number of documents by the number of documents containing the\n",
    "term, and then taking the logarithm of that quotient.\n",
    "\n",
    ":$ \\mathrm{idf}(t, D) = \\log \\frac{|D|}{|\\{d \\in D: t \\in d\\}|}$\n",
    "\n",
    "with\n",
    "\n",
    "  * $ |D| $: cardinality of D, or the total number of documents in the corpus \n",
    "  * $ |\\{d \\in D: t \\in d\\}| $ : number of documents where the term $ t $ appears (i.e., $ \\mathrm{tf}(t,d) eq 0$). If the term is not in the corpus, this will lead to a division-by-zero. It is therefore common to adjust the formula to $1 + |\\{d \\in D: t \\in d\\}|$. \n",
    "\n",
    "Mathematically the base of the log function does not matter and constitutes a\n",
    "constant multiplicative factor towards the overall result.\n",
    "\n",
    "Then tf–idf is calculated as\n",
    "\n",
    "$$\\mathrm{tfidf}(t,d,D) = \\mathrm{tf}(t,d) \\times \\mathrm{idf}(t, D)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "\n",
    "vectorizer = text.CountVectorizer(max_df=0.8, max_features=10000, stop_words=text.ENGLISH_STOP_WORDS)\n",
    "counts = vectorizer.fit_transform(dataset.data)\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing with NLTK\n",
    "\n",
    "NLTK (Natural Language ToolKit) is a library for symbolic and statistical natural language processing (NLP).\n",
    "\n",
    "It supports a few functionalities for NLP, such as:\n",
    "\n",
    "- Lexical analysis: Word and text tokenizer\n",
    "- n-gram and collocations\n",
    "- Part-of-speech tagger\n",
    "- Tree model and Text chunker for capturing\n",
    "- Named-entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import stem\n",
    "import re\n",
    "\n",
    "pattern = re.compile('(?u)\\\\b[A-Za-z]{3,}')\n",
    "\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "def stemming(doc):\n",
    "    l = [stemmer.stem(t) for t in pattern.findall(doc)]\n",
    "    return [w for w in l if len(w) > 2]\n",
    "\n",
    "wnl = stem.WordNetLemmatizer()\n",
    "def lemmatize(doc):\n",
    "    \n",
    "    def lemma(w):\n",
    "        l = wnl.lemmatize(t, wn.NOUN)\n",
    "        if l == w:\n",
    "            l = wnl.lemmatize(t, wn.ADJ)\n",
    "        if l == w:\n",
    "            l = wnl.lemmatize(t, wn.ADV)\n",
    "        if l == w:\n",
    "            l = wnl.lemmatize(t, wn.VERB)\n",
    "        return l\n",
    "    \n",
    "    l = [lemma(t) for t in pattern.findall(doc)]\n",
    "    return [w for w in l if len(w) > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence = \"\"\"For grammatical reasons, documents are going to use different forms of a word,\n",
    "such as organize, organizes, and organizing.\n",
    "Additionally, there are families of derivationally related words with similar meanings,\n",
    "such as democracy, democratic, and democratization.\"\"\"\n",
    "\n",
    "print sentence\n",
    "print\n",
    "print stemming(sentence)\n",
    "print\n",
    "print lemmatize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = text.CountVectorizer(max_df=0.95, max_features=10000, stop_words='english',\n",
    "                                  encoding='latin1', tokenizer=lemmatize, ngram_range=(1, 2))\n",
    "counts = vectorizer.fit_transform(dataset.data)\n",
    "tfidf = text.TfidfTransformer().fit_transform(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we create a variable, tfidf, which is a vectorizer responsible for performing three important steps:\n",
    "\n",
    "- First, it will build a dictionary of features where keys are terms and values are indices of the term in the feature matrix (that's the fit part in fit_transform)\n",
    "- Second, it will transform our documents into numerical feature vectors according to the frequency of words appearing in each text message. Since any one text message is short, each feature vector will be made up of mostly zeros, each of which indicates that a given word appeared zero times in that message.\n",
    "- Lastly, it will compute the tf-idf weights for our term frequency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonnegative Matrix Factorization for Topic extraction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine having 5 documents, 2 of them about environment and 2 of them about U.S. Congress and 1 about both, that means it says about government legislation process in protecting an environment. We need to write a program that unmistakably identifies category of each document and also returns a degree of belonging of each document to a particular category. For this elementary example we limit our vocabulary to 5 words: AIR, WATER, POLLUTION, DEMOCRAT, REPUBLICAN. Category ENVIRONMENT and category CONGRESS may contain all 5 words but with different probability. We understand that the word POLLUTION has more chances to be in the article about ENVIRONMENT than in the article about CONGRESS, but can theoretically be in both. Presume after an examination of our data we built following document-term table:\n",
    "\n",
    "<table border=\"\" cellpadding=\"3\" style=\"font-family:'Times New Roman'\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>document/word</td>\n",
    "<td>air</td>\n",
    "<td>water</td>\n",
    "<td>pollution</td>\n",
    "<td>democrat</td>\n",
    "<td>republican</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 1</td>\n",
    "<td>3</td>\n",
    "<td>2</td>\n",
    "<td>8</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 2</td>\n",
    "<td>1</td>\n",
    "<td>4</td>\n",
    "<td>12</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 3</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>10</td>\n",
    "<td>11</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 4</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>8</td>\n",
    "<td>5</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 5</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "<td>1</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "We distinguish our categories by the group of words assigned to them. We decide that category ENVIRONMENT normally should contain only words AIR, WATER, POLLUTION and category CONGRESS should contain only words DEMOCRAT and REPUBLICAN. We build another matrix, each row of which represent category and contains counts for only words that assigned to each category. \n",
    "\n",
    "<table border=\"\" cellpadding=\"3\" style=\"font-family:'Times New Roman'\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>categories</td>\n",
    "<td>air</td>\n",
    "<td>water</td>\n",
    "<td>pollution</td>\n",
    "<td>democrat</td>\n",
    "<td>republican</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ENVIRONMENT</td>\n",
    "<td>5</td>\n",
    "<td>7</td>\n",
    "<td>21</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CONGRESS</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>19</td>\n",
    "<td>17</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "We change values from frequencies to probabilities by dividing them by sums in rows, which turns each row into probability distribution.\n",
    "\n",
    "<table border=\"\" cellpadding=\"3\" style=\"font-family:'Times New Roman'\">\n",
    "<caption>Matrix&nbsp;<strong>H</strong></caption>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>categories</td>\n",
    "<td>air</td>\n",
    "<td>water</td>\n",
    "<td>pollution</td>\n",
    "<td>democrat</td>\n",
    "<td>republican</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>ENVIRONMENT</td>\n",
    "<td>0.15</td>\n",
    "<td>0.21</td>\n",
    "<td>0.64</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>CONGRESS</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0.53</td>\n",
    "<td>0.47</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Now we create another matrix that contains probability distribution for categories within each document that looks like follows:\n",
    "\n",
    "<table border=\"\" cellpadding=\"3\" style=\"font-family:'Times New Roman'\">\n",
    "<caption>Matrix&nbsp;<strong>W</strong></caption>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>documents</td>\n",
    "<td>ENVIRONMENT</td>\n",
    "<td>CONGRESS</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 1</td>\n",
    "<td>1.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 2</td>\n",
    "<td>1.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 3</td>\n",
    "<td>0.0</td>\n",
    "<td>1.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 4</td>\n",
    "<td>0.0</td>\n",
    "<td>1.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>doc 5</td>\n",
    "<td>0.6</td>\n",
    "<td>0.4</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "It shows that top two documents speak about environment, next two about congress and last document about both. Ratios 0.6 and 0.4 for the last document are defined by 3 words from environment category and 2 words from congress category. Now we multiply both matrices and compare the result with original data but in a normalized form. Normalization in this case is division of each row by the sum of all elements in rows. The comparison is shown side-by-side below:\n",
    "\n",
    "<table cellpadding=\"10\" style=\"font-family:'Times New Roman'\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>\n",
    "<table border=\"\" cellpadding=\"3\">\n",
    "<caption>Product of&nbsp;<strong>W * H</strong></caption>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0.15</td>\n",
    "<td>0.21</td>\n",
    "<td>0.64</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.15</td>\n",
    "<td>0.21</td>\n",
    "<td>0.64</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.53</td>\n",
    "<td>0.47</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.53</td>\n",
    "<td>0.47</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.09</td>\n",
    "<td>0.13</td>\n",
    "<td>0.38</td>\n",
    "<td>0.21</td>\n",
    "<td>0.19</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</td>\n",
    "<td>\n",
    "<table border=\"\" cellpadding=\"3\">\n",
    "<caption>Normalized data&nbsp;<strong>N</strong></caption>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>0.23</td>\n",
    "<td>0.15</td>\n",
    "<td>0.62</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.06</td>\n",
    "<td>0.24</td>\n",
    "<td>0.70</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.48</td>\n",
    "<td>0.52</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.0</td>\n",
    "<td>0.61</td>\n",
    "<td>0.39</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "<td>0.2</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "The correlation is obvious. The problem definition is to find constrained matrices W and H (given the number of categories), product of which is the best match with normalized data N. When approximation is found matrix H will contain sought categories.\n",
    "\n",
    "**Formally**, we are trying to minimize this:\n",
    "\n",
    "$$ \\|\\mathbf{N} - \\mathbf{WH}\\|^2_F $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition\n",
    "\n",
    "# Fit the NMF model\n",
    "nmf = decomposition.NMF(n_components=6)\n",
    "nmf.fit(tfidf)\n",
    "W = nmf.transform(tfidf)\n",
    "H = nmf.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Inverse the vectorizer vocabulary to be able\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for topic_idx, topic in enumerate(H):\n",
    "    print \"Topic #%d:\" % topic_idx\n",
    "    print \",\".join([feature_names[i] for i in topic.argsort()[:-21:-1]])\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise\n",
    "\n",
    "Compare the results of Nonnegative Matrix Factorization (NMF) with Latent Dirichlet Allocation (LDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
