{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn import grid_search, linear_model, naive_bayes, svm, preprocessing, cross_validation\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import stem\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv.gz\", compression=\"gzip\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ejchristian86</td>\n",
       "      <td>TwoXChromosomes</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I hadn't ever heard of them before joining thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shamus_Aran</td>\n",
       "      <td>mylittlepony</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>I don't think we'd get nearly as much fanficti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Riddance</td>\n",
       "      <td>sex</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Thanks. I made it up, that's how I got over my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>savoytruffle</td>\n",
       "      <td>AskReddit</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>bite me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Secret_Wizard</td>\n",
       "      <td>DragonsDogma</td>\n",
       "      <td>1.388534e+09</td>\n",
       "      <td>Are you sure you aren't confusing Cyclops (the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          author        subreddit   created_utc  \\\n",
       "0  ejchristian86  TwoXChromosomes  1.388534e+09   \n",
       "1    Shamus_Aran     mylittlepony  1.388534e+09   \n",
       "2       Riddance              sex  1.388534e+09   \n",
       "3   savoytruffle        AskReddit  1.388534e+09   \n",
       "4  Secret_Wizard     DragonsDogma  1.388534e+09   \n",
       "\n",
       "                                                body  \n",
       "0  I hadn't ever heard of them before joining thi...  \n",
       "1  I don't think we'd get nearly as much fanficti...  \n",
       "2  Thanks. I made it up, that's how I got over my...  \n",
       "3                                            bite me  \n",
       "4  Are you sure you aren't confusing Cyclops (the...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"train_target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RedThunder90</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lirkmor</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>In0chi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ProjectGrudge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TehTurtleHermit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            author  gender\n",
       "0     RedThunder90       0\n",
       "1          Lirkmor       1\n",
       "2           In0chi       0\n",
       "3    ProjectGrudge       0\n",
       "4  TehTurtleHermit       0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subreddits = train_data.subreddit.unique()\n",
    "subreddits_map = pd.Series(index=subreddits, data=arange(subreddits.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pattern = re.compile('(?u)\\\\b((([a-z])(?!\\\\3{2,})){4,})\\\\b')\n",
    "\n",
    "stemmer = stem.SnowballStemmer('english')\n",
    "def stemming(doc):\n",
    "    split = []\n",
    "    for word in pattern.finditer(doc):\n",
    "        split.append(word.group())\n",
    "    l = [stemmer.stem(t) for t in split]\n",
    "    return [w for w in l if len(w) > 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_data.csv.gz\", compression=\"gzip\", encoding=\"utf8\")\n",
    "test_data = test_data[[\"author\", \"body\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 17.9 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "                     \n",
    "def make_corpus(train, test):\n",
    "    for author, group in train.groupby('author'):\n",
    "        yield \" \".join(group[\"body\"].values.astype('U'))\n",
    "    for author, group in test.groupby('author'):\n",
    "        yield \" \".join(group[\"body\"].values.astype('U'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 42s, sys: 9.58 s, total: 36min 52s\n",
      "Wall time: 36min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vectorizer = text.TfidfVectorizer(min_df=10, max_df=0.9, stop_words='english', decode_error='ignore',\n",
    "                                   ngram_range=(1, 1), sublinear_tf=True, tokenizer = stemming)\n",
    "vectorizer.fit(make_corpus(train_data, test_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37440"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7min 44s, sys: 368 ms, total: 7min 44s\n",
      "Wall time: 7min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "del test_data\n",
    "\n",
    "words_dict = {}\n",
    "\n",
    "for author, group in train_data.groupby('author'):\n",
    "    words_dict[author] = \" \".join(group[\"body\"].values)\n",
    "\n",
    "aut_bodies = [words_dict[author] for author in target.author]\n",
    "\n",
    "del words_dict\n",
    "\n",
    "X_words = vectorizer.transform(aut_bodies)\n",
    "\n",
    "del aut_bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = target.gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 5.6234132519034912e-06}\n",
      "0.84208282212\n",
      "CPU times: user 16.6 s, sys: 300 ms, total: 16.9 s\n",
      "Wall time: 9.12 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "word_alphas = np.logspace(-6, -5, 5)\n",
    "\n",
    "word_gs = grid_search.GridSearchCV(word_model, {\"alpha\":word_alphas}, cv=7, scoring='roc_auc')\n",
    "word_gs.fit(X_words, y)\n",
    "print word_gs.best_params_\n",
    "print word_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del X_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          0.          1.          1.          1.          1.          1.\n",
      "  1.          1.          1.          0.          1.          1.          0.3398721\n",
      "  1.          0.          0.          1.          1.          1.          1.\n",
      "  0.          0.          1.          1.          0.          1.\n",
      "  0.60823849  1.          0.69676993  1.          0.21533796  0.38792112\n",
      "  1.          1.          0.          1.          0.17612916  1.          0.\n",
      "  1.          0.          1.          1.          1.          0.\n",
      "  0.74566946  1.          0.          1.          0.25666091  1.          1.\n",
      "  0.          1.          0.71338505  0.          0.          1.          1.        ]\n",
      "  (0, 3)\t1.0\n",
      "  (0, 18)\t1.0\n",
      "  (0, 225)\t1.0\n",
      "  (0, 619)\t1.0\n",
      "  (0, 1399)\t1.0\n",
      "  (0, 2785)\t1.0\n",
      "  (0, 3866)\t0.668999736213\n"
     ]
    }
   ],
   "source": [
    "def extract_features(group):\n",
    "    group_subreddits = group['subreddit'].values\n",
    "    idxs = subreddits_map[group_subreddits].values\n",
    "    v = sparse.dok_matrix((1, subreddits.shape[0]))\n",
    "    for idx in idxs:\n",
    "        if not np.isnan(idx):\n",
    "            v[0, idx] = 1.0\n",
    "    v3 = vectorizer.transform(group[\"body\"].values.astype('U'))\n",
    "    return sparse.hstack([v.tocsr(), word_model.predict_proba(v3)[:,1].mean()]) \n",
    "\n",
    "print extract_features(train_data[train_data.author=='LadyWhiskers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 28s, sys: 1.25 s, total: 9min 29s\n",
      "Wall time: 9min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "features_dict = {}\n",
    "\n",
    "for author, group in train_data.groupby('author'):\n",
    "    features_dict[author] = extract_features(group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sparse.vstack([features_dict[author] for author in target.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del features_dict\n",
    "y = target.gender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.4517241379310345}\n",
      "0.931080908041\n",
      "CPU times: user 4.7 s, sys: 4 ms, total: 4.71 s\n",
      "Wall time: 4.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "model = naive_bayes.MultinomialNB()\n",
    "\n",
    "alphas = np.linspace(0.3, 0.5, 30)\n",
    "\n",
    "gs = grid_search.GridSearchCV(model, {\"alpha\":alphas}, cv=9, scoring='roc_auc')\n",
    "gs.fit(X, y)\n",
    "print gs.best_params_\n",
    "print gs.best_score_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test_data.csv.gz\", compression=\"gzip\", encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del train_data\n",
    "del target\n",
    "test_data['body'].fillna('', inplace=True)\n",
    "aut_uniq = test_data.author.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "features_dict = {}\n",
    "\n",
    "for author, group in test_data.groupby('author'):\n",
    "    features_dict[author] = extract_features(group)\n",
    "    \n",
    "del test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test = sparse.vstack([features_dict[author] for author in aut_uniq])\n",
    "del features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = gs.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asks_Politely</td>\n",
       "      <td>1.814081e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>smartphone-redditor</td>\n",
       "      <td>1.336787e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Simcom</td>\n",
       "      <td>5.099409e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZenDragon</td>\n",
       "      <td>1.011557e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>imgurtranscriber</td>\n",
       "      <td>2.698285e-61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author        gender\n",
       "0        Asks_Politely  1.814081e-09\n",
       "1  smartphone-redditor  1.336787e-02\n",
       "2               Simcom  5.099409e-12\n",
       "3            ZenDragon  1.011557e-10\n",
       "4     imgurtranscriber  2.698285e-61"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "solution = pd.DataFrame({\"author\":aut_uniq, \"gender\":y_pred})\n",
    "solution.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "solution.to_csv(\"words_classifier.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
